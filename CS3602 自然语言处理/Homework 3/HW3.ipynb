{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业三：预训练语言模型计算PPL\n",
    "姓名：薛翔元\n",
    "学号：521030910387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "下面是一个例子，展示Tokenizer和模型的使用。理解下面的例子可能对你的大作业有帮助。\n",
    "\n",
    "Tokenizer会将句子分割成一个个token，然后将每个token转化为一个数字，这个数字就是这个token在词表中的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   38, 11571,    12,    17,   318,   257,  6121,   364,  2746,  2181,\n",
       "         13363,   319,   257,   845,  1588, 35789,   286,  3594,  1366,   287,\n",
       "           257,  2116,    12, 16668, 16149,  6977,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"\"\"GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion.\"\"\", return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将token id映射到对应的分词token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'PT', '-', '2', 'Ġis', 'Ġa', 'Ġtransform', 'ers', 'Ġmodel', 'Ġpret', 'rained', 'Ġon', 'Ġa', 'Ġvery', 'Ġlarge', 'Ġcorpus', 'Ġof', 'ĠEnglish', 'Ġdata', 'Ġin', 'Ġa', 'Ġself', '-', 'super', 'vised', 'Ġfashion', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用`decode`方法将token id转化回原来的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion.\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([38, 11571, 12, 17, 318, 257, 6121, 364, 2746, 2181, 13363, 319, 257, 845, 1588, 35789, 286, 3594, 1366, 287, 257, 2116, 12, 16668, 16149, 6977, 13])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2\n",
    "\n",
    "GPT2是自回归式语言模型，可以根据前面的token预测下一个token。\n",
    "\n",
    "将上面的token id输入到GPT2模型中，就可以得到每个token的概率分布\n",
    "\n",
    "GPT2的输出的logits是一个三维张量，第一维是batch size，第二维是token的数量，第三维是词表的大小\n",
    "\n",
    "> 注意：GPT2输出的是logits，需要经过softmax才能得到真正的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 50257])\n",
      "tensor([-31.8240, -31.4345, -33.4860,  ..., -39.5280, -38.9087, -31.8361],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs.input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits\n",
    "print(logits.shape) # batch大小，序列长度，词表大小\n",
    "print(logits[0, 0, :]) # 对于第一个词的预测logits，通过softmax后可以得到概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算Perplexity (PPL)\n",
    "\n",
    "PPL是语言模型的一个重要评价指标，表示模型对于给定的句子的概率分布的拟合程度。\n",
    "\n",
    "计算公式为：\n",
    "$$\n",
    "PPL = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1,w_2,...,w_{i-1})}}\n",
    "$$\n",
    "通常可以转化为对数形式：\n",
    "$$\n",
    "PPL = \\exp\\left(\\frac{1}{N}\\sum_{i=1}^{N}-\\log P(w_i|w_1,w_2,...,w_{i-1})\\right)\n",
    "$$\n",
    "\n",
    "本节将实现GPT2模型的PPL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Softmax, CrossEntropyLoss\n",
    "\n",
    "\n",
    "def calculate_ppl(model, text):\n",
    "    ## TODO: 首先将文本转换为输入token (7分)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    # 获取模型的输出\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "        labels = input_ids.to(logits.device)\n",
    "        # GPT2每个位置都是预测下一个token的概率，所以需要将labels向左移动一位\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "        ## TODO: 根据logits和labels计算model在text上的ppl（8分）\n",
    "        ## Hint: 可以直接通过Softmax获取概率值按照上面公式计算\n",
    "        ## Hint2: 也可以尝试利用CrossEntropyLoss进行等价计算\n",
    "        loss_fn = CrossEntropyLoss(reduction='mean')\n",
    "        loss = loss_fn(shift_logits.squeeze(0), shift_labels.squeeze(0))\n",
    "        ppl = torch.exp(loss).item()\n",
    "        # A plain but equivalent method here\n",
    "        # ppl = 0\n",
    "        # shift_logits = Softmax(dim=-1)(shift_logits)\n",
    "        # for i in range(len(shift_logits[0])):\n",
    "        #     ppl += torch.log(shift_logits[0, i, shift_labels[0, i]])\n",
    "        # ppl = torch.exp(-ppl / len(shift_logits[0])).item()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.2030258178711\n",
      "46.45439910888672\n"
     ]
    }
   ],
   "source": [
    "text1 = \"GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.\"\n",
    "text2 = \"Until the rocket ship nearly imploded. On Nov. 17, OpenAI's nonprofit board of directors fired Altman, without warning or even much in the way of explanation. The surreal maneuvering that followed made the corporate dramas of Succession seem staid. Employees revolted. So did OpenAI's powerful investors; one even baselessly speculated that one of the directors who defenestrated Altman was a Chinese spy. The company's visionary chief scientist voted to oust his fellow co-founder, only to backtrack. Two interim CEOs came and went. The players postured via selfie, open letter, and heart emojis on social media. Meanwhile, the company's employees and its board of directors faced off in “a gigantic game of chicken,” says a person familiar with the discussions. At one point, OpenAI's whole staff threatened to quit if the board didn't resign and reinstall Altman within a few hours, three people involved in the standoff tell TIME. Then Altman looked set to decamp to Microsoft—with potentially hundreds of colleagues in tow. It seemed as if the company that catalyzed the AI boom might collapse overnight.\"\n",
    "\n",
    "print(calculate_ppl(model, text1))\n",
    "print(calculate_ppl(model, text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（TODO：实验总结）\n",
    "\n",
    "> 根据 PPL 计算公式，我们进行如下变换\n",
    ">\n",
    "> $$\\begin{aligned} \\text{PPL} &= \\exp \\left( - \\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_1, w_2, ..., w_{i-1}) \\right) \\\\ &= \\exp \\left( - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp \\left( \\text{logits}(w_i) \\right)}{\\sum_{j=1}^M \\exp \\left( \\text{logits}(w_j) \\right)} \\right) \\\\ &= \\exp \\left( \\frac{1}{N} \\sum_{i=1}^{N} - \\log \\text{Softmax} \\left( \\text{logits} (w_i) \\right) \\right) \\\\ &= \\exp \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\text{CrossEntropy} \\left( \\text{logits} (w_i) | \\text{onehot}(w_i) \\right) \\right) \\end{aligned}$$\n",
    ">\n",
    "> 因此，内层可以直接使用 `CrossEntropyLoss` 进行计算，最后再取指数即可\n",
    ">\n",
    "> 此外，注释中展示了基于循环的朴素实现，两种方法得到的结果一致，验证了实现的正确性\n",
    ">\n",
    "> 测试结果表明，GPT2 模型的 PPL 相对较小，具有较强的语言生成能力"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
